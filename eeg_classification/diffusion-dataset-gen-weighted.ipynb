{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Tensor\n",
    "import bisect\n",
    "import csv\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common.dog import DoG, LDoG, PDoG\n",
    "# from cnn_models import model_size\n",
    "# from cnn_models import CNNClassifierLight, ResNet, LabelSmoothingCrossEntropy\n",
    "# from cnn_models import EfficientNet, ShuffleNet, ResNet\n",
    "from data_processing.math import MathDataset\n",
    "from data_processing.parkinsons import ParkinsonsDataset, health_class_labels, HealthSampler\n",
    "from data_processing.seed import SEEDDataset, emotion_class_labels, EmotionSampler\n",
    "from data_processing.general_dataset import GeneralPreprocessor, GeneralDataset, GeneralSampler\n",
    "from data_processing.general_dataset import general_class_labels, general_dataset_map\n",
    "# from training import train_class, evaluate_class, TrainingConfig\n",
    "# from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 205 #205 Gave a good split for training\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datapaths\n",
    "datadirs = {}\n",
    "datahome = '/data/shared/signal-diffusion'\n",
    "# datahome = '/mnt/d/data/signal-diffusion'\n",
    "\n",
    "# Math dataset\n",
    "datadirs['math'] = f'{datahome}/eeg_math'\n",
    "datadirs['math-stft'] = os.path.join(datadirs['math'], 'stfts')\n",
    "\n",
    "# Parkinsons dataset\n",
    "datadirs['parkinsons'] = f'{datahome}/parkinsons/'\n",
    "datadirs['parkinsons-stft'] = os.path.join(datadirs['parkinsons'], 'stfts')\n",
    "\n",
    "#SEED dataset\n",
    "datadirs['seed'] = f'{datahome}/seed/'\n",
    "datadirs['seed-stft'] = os.path.join(datadirs['seed'], \"stfts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps = 2000\n",
    "\n",
    "preprocessor = GeneralPreprocessor(datadirs, nsamps, ovr_perc=0.5, fs=125, bin_spacing=\"log\")\n",
    "preprocessor.preprocess(resolution=256, train_frac=0.8, val_frac=0.2, test_frac=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "parkinsons_real_train_dataset = ParkinsonsDataset(datadirs['parkinsons-stft'], split=\"train\", transform=None)\n",
    "seed_real_train_dataset = SEEDDataset(datadirs['seed-stft'], split=\"train\")\n",
    "real_train_datasets = [parkinsons_real_train_dataset, seed_real_train_dataset]\n",
    "real_train_set = GeneralDataset(real_train_datasets, split='train')\n",
    "train_samp = GeneralSampler(real_train_datasets, BATCH_SIZE, split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(seed_real_train_dataset))\n",
    "print(len(parkinsons_real_train_dataset))\n",
    "print(len(real_train_set))\n",
    "print(len(train_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = train_samp.weights / torch.min(train_samp.weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(weights.numpy()))\n",
    "print(sum(weights.numpy()))\n",
    "for wgt in set(weights.numpy()):\n",
    "    print(f\"{wgt:.4f}: add {np.ceil(wgt)} every {np.floor(1/(wgt - np.floor(wgt) + 1e-5))} else {np.floor(wgt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Data\n",
    "\n",
    "In order to have a class-balanced version on-disk.\n",
    "\n",
    "Select data indices by weight, then add an extra copy every i'th (or skip copy every i'th) to match fractional weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_copies(idxs, metadata, out_dir, copy_fn):\n",
    "    count = 0\n",
    "    for i in idxs:\n",
    "        i = i.item()\n",
    "        # Get dataset & index\n",
    "        dataset_idx = bisect.bisect_right(real_train_set.cumulative_sizes, i)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = i\n",
    "        else:\n",
    "            sample_idx = i - real_train_set.cumulative_sizes[dataset_idx - 1]\n",
    "        dataset = real_train_set.datasets[dataset_idx]\n",
    "        # Get metadata\n",
    "        md = dataset.metadata.iloc[sample_idx]\n",
    "        # Make output directory\n",
    "        os.makedirs(os.path.dirname(os.path.join(out_dir, dataset.name, md['file_name'])), exist_ok=True)\n",
    "        # Number of copies\n",
    "        copies = copy_fn(count)\n",
    "        # Save copies\n",
    "        for c in range(copies):\n",
    "            fn = md['file_name']\n",
    "            new_fn = f'{dataset.name}/{fn[:-4]}_{c}.png'\n",
    "            mdc = md.copy()\n",
    "            mdc['file_name'] = new_fn\n",
    "            metadata.append(mdc)\n",
    "            shutil.copyfile(os.path.join(dataset.datadir, fn), os.path.join(out_dir, new_fn))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = f'{datahome}/reweighted_meta_dataset'\n",
    "metadata = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0124 - add a 3rd copy every 80 samples\n",
    "idxs = torch.argwhere(torch.logical_and(weights > 2, weights < 2.02)).reshape(-1)\n",
    "copy_fn = lambda count: 3 if count % 80 == 0 else 2\n",
    "make_copies(idxs, metadata, out_dir, copy_fn)\n",
    "print(len(metadata))\n",
    "print(metadata[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0386 - add a 3rd copy every 25 samples\n",
    "idxs = torch.argwhere(torch.logical_and(weights > 2.02, weights < 2.1)).reshape(-1)\n",
    "copy_fn = lambda count: 3 if count % 25 == 0 else 2\n",
    "make_copies(idxs, metadata, out_dir, copy_fn)\n",
    "print(len(metadata))\n",
    "print(metadata[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - add 1 copy always\n",
    "idxs = torch.argwhere(torch.logical_and(weights > 0.9, weights < 1.01)).reshape(-1)\n",
    "copy_fn = lambda count: 1\n",
    "make_copies(idxs, metadata, out_dir, copy_fn)\n",
    "print(len(metadata))\n",
    "print(metadata[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0262 - add 2 copies every 38 samples\n",
    "idxs = torch.argwhere(torch.logical_and(weights > 1.01, weights < 1.1)).reshape(-1)\n",
    "copy_fn = lambda count: 2 if count % 38 == 0 else 1\n",
    "make_copies(idxs, metadata, out_dir, copy_fn)\n",
    "print(len(metadata))\n",
    "print(metadata[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "metapd = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metapd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metapd.to_csv(os.path.join(out_dir, 'metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
