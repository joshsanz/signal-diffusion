{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from threading import Lock\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional, Union\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torchvision import transforms\n",
    "\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, GradScalerKwargs, set_seed\n",
    "\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "\n",
    "import transformers\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import EMAModel\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers import Mel\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_vae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = VAETrainConfig(\n",
    "    mixed_precision='no',\n",
    "    discriminator_start=1000,\n",
    "    num_train_epochs=5,\n",
    "    checkpoints_total_limit=5,\n",
    "    checkpointing_steps=1000,\n",
    "    resume_from_checkpoint=None,\n",
    "    output_dir=\"vae-stft-fma-2\",\n",
    ")\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from contperceptual_loss import LPIPSWithDiscriminator\n",
    "vae0 = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision).to('cuda')\n",
    "vae0.encoder.requires_grad_(False)\n",
    "vae0.train()\n",
    "disc_start = 10  # Should be more like 1000 for real training\n",
    "loss_model = LPIPSWithDiscriminator(disc_start, logvar_init=0.0, kl_weight=1.0, pixelloss_weight=1.0,\n",
    "                 disc_num_layers=3, disc_in_channels=3, disc_factor=1.0, disc_weight=1.0,\n",
    "                 perceptual_weight=1.0, use_actnorm=False, disc_conditional=False,\n",
    "                 disc_loss=\"hinge\").to('cuda')\n",
    "loss_model.train();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accelerator = Accelerator(mixed_precision=args.mixed_precision,)\n",
    "dataset, dataloader = get_dataloader(accelerator, args)\n",
    "print(f\"{len(dataloader)} images available\")\n",
    "data_iter = enumerate(dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "step, batch = next(data_iter)\n",
    "batch['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_model_size(loss_model.perceptual_loss, verbose=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "step, batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "loss_model.to('cuda')\n",
    "vae0.to('cuda')\n",
    "original = batch['pixel_values'].to('cuda')\n",
    "latent_dist = vae0.encode(original).latent_dist\n",
    "reconstruction = vae0.decode(latent_dist.sample()).sample\n",
    "loss_percep, log_percep = loss_model(original, reconstruction, latent_dist, 0,\n",
    "                                     step, last_layer=vae0.decoder.conv_out.weight, cond=None, split=\"train\",\n",
    "                                     weights=None)\n",
    "loss_discrim, log_discrim = loss_model(original, reconstruction, latent_dist, 1,\n",
    "                                       step, last_layer=vae0.decoder.conv_out.weight, cond=None, split=\"train\",\n",
    "                                       weights=None)\n",
    "loss_percep.backward()\n",
    "loss_discrim.backward()\n",
    "print(\"-\"*20)\n",
    "print(log_percep)\n",
    "print(\"-\"*20)\n",
    "print(log_discrim)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fjkdsl;a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# accelerate.notebook_launcher(train, [args], num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae0 = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision).to('cuda')\n",
    "vae1 = AutoencoderKL.from_pretrained(args.output_dir , subfolder=\"\", revision=args.revision).to('cuda')\n",
    "vae_ckpts = [AutoencoderKL.from_pretrained(args.output_dir + \"/checkpoints/\" + sf, subfolder=\"vae\", revision=args.revision).to('cuda')\n",
    "             for sf in os.listdir(args.output_dir + \"/checkpoints\")]\n",
    "all_vaes = [vae0, vae1] + vae_ckpts\n",
    "for vae in all_vaes:\n",
    "    vae.requires_grad_(False)\n",
    "    vae.eval()\n",
    "print(f\"Model size {get_model_size(vae0):.3f} MB (2x)\")\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=args.mixed_precision,)\n",
    "dataset, dataloader = get_dataloader(accelerator, args)\n",
    "print(f\"{len(dataloader)} images available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STFT Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "image, latents = encode_sample(vae0, dataset[idx])\n",
    "recons = [decode_latents(vae, latents) for vae in all_vaes]\n",
    "imgs = [numpy_to_pil(recon)[0] for recon in recons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid = image_grid([image, *imgs], 4, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 512\n",
    "fs = 22050\n",
    "mel = Mel(x_res=resolution, y_res=resolution, sample_rate=fs, n_fft=2048,\n",
    "          hop_length=resolution, top_db=80, n_iter=32,)\n",
    "images = [image, *imgs]\n",
    "audios = [mel.image_to_audio(im.convert('L')) for im in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aud in audios:\n",
    "    ipd.display(ipd.Audio(aud, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_mse(audios):\n",
    "    a0 = audios[0]\n",
    "    mses = np.zeros(len(audios) - 1)\n",
    "    for i, ai in enumerate(audios[1:]):\n",
    "        mses[i] = np.linalg.norm((a0 - ai)) ** 2\n",
    "    return mses\n",
    "\n",
    "mses = audio_mse(audios)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cargs = copy.copy(args)\n",
    "cargs.train_batch_size = 2\n",
    "dataset, dataloader = get_dataloader(accelerator, cargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "recons = []\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    imgs, latents = encode_sample(vae0, batch)\n",
    "    images.append(imgs)\n",
    "    for vae in all_vaes:\n",
    "        recon = decode_latents(vae, latents)\n",
    "        recons.append(recon)\n",
    "    print(\"Finished\", i)\n",
    "\n",
    "len(recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nvaes = len(all_vaes)\n",
    "nrecons = len(recons) // nvaes\n",
    "vae_recons = [[recons[6 * i + j] for i in range(nrecons)] for j in range(nvaes)]\n",
    "print(len(vae_recons))\n",
    "print(len(vae_recons[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_images = []\n",
    "for ivae in range(nvaes):\n",
    "    vae_images.append([])\n",
    "    for recon in vae_recons[ivae]:\n",
    "        vae_images[ivae].extend(numpy_to_pil(recon))\n",
    "\n",
    "pil_images = []\n",
    "for imgs in images:\n",
    "    pil_images.extend(imgs)\n",
    "    \n",
    "print(len(vae_images))\n",
    "print(len(vae_images[0]))\n",
    "print(len(pil_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resolution = 512\n",
    "fs = 22050\n",
    "mel = Mel(x_res=resolution, y_res=resolution, sample_rate=fs, n_fft=2048,\n",
    "          hop_length=resolution, top_db=80, n_iter=32,)\n",
    "vae_audios = []\n",
    "for ivae in range(nvaes):\n",
    "    vae_audios.append(None)\n",
    "    vae_audios[ivae] = [mel.image_to_audio(im.convert('L')) for im in vae_images[ivae]]\n",
    "    print(\"Finished\", ivae)\n",
    "\n",
    "orig_audios = []\n",
    "for pim in pil_images:\n",
    "    orig_audios.append(mel.image_to_audio(pim.convert('L')))\n",
    "\n",
    "print(len(vae_audios))\n",
    "print(len(vae_audios[0]))\n",
    "print(len(orig_audios))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.savez(\"audio_reconstruction.pkl\", aorig=orig_audios, iorig=pil_images, avae=vae_audios, ivae=vae_images)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = np.load(\"audio_reconstruction.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = []\n",
    "for zaudio in zip(orig_audios, *vae_audios):\n",
    "    mses.append(audio_mse(zaudio))\n",
    "mses = np.array(mses)\n",
    "plt.errorbar(np.arange(6), np.mean(mses, axis=0), yerr=np.std(mses, axis=0), linestyle='none')\n",
    "plt.title(\"VAE vs Audio Reconstruction MSE\")\n",
    "plt.xlabel(\"VAE index [0=default, 1=final, 2+=checkpoint]\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
