{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798b6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Tensor\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ebe454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls 'gdrive/My Drive/Muller Group Drive/Ear EEG/Drowsiness_Detection/classifier_TBME'\n",
    "#!ls C:\\Users\\arya_bastani\\Documents\\ear_eeg\\data\\ear_eeg_data\n",
    "ear_eeg_base_path = '/data/shared/signal-diffusion/'\n",
    "ear_eeg_data_path = ear_eeg_base_path + 'eeg_classification_data/ear_eeg_data/ear_eeg_clean'\n",
    "\n",
    "%ls {ear_eeg_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "#import support scripts: pull_data\n",
    "import support_scripts.read_in_ear_eeg as read_in_ear_eeg\n",
    "import support_scripts.read_in_labels as read_in_labels\n",
    "import support_scripts.eeg_filter as eeg_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d06940",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# READ-IN EAR EEG\n",
    "##################\n",
    "# NOTE, this takes a long time to run.\n",
    "# (It could be parallelized to reduce runtime)\n",
    "\n",
    "# name of spreadsheet with experiment details\n",
    "# details_spreadsheet = 'gdrive/My Drive/Muller Group Drive/Ear EEG/Drowsiness_Detection/classifier_TBME/classification_scripts/trial_details_spreadsheet_basic.csv'\n",
    "details_spreadsheet = ear_eeg_base_path + 'eeg_classification_data/ear_eeg_data/trial_details_spreadsheet_good.csv'\n",
    "\n",
    "# file path to ear eeg data (must be formated r'filepath\\\\')\n",
    "#data_filepath = r'C:\\Users\\Carolyn\\OneDrive\\Documents\\school\\berkeley\\research\\ear_eeg_classification_framework\\experimental_recordings\\drowsiness_studies\\ear_eeg\\\\'\n",
    "data_filepath = ear_eeg_base_path + 'eeg_classification_data/ear_eeg_data/ear_eeg_clean/'\n",
    "\n",
    "# user number or all users('all', 'ryan', 'justin', 'carolyn', 'ashwin', 'connor')\n",
    "input_users = 'all'\n",
    "\n",
    "# channels of eeg to read in for each trial (must include 5 and 11 if re-refernecing is enabled in the next block)\n",
    "data_chs = [1,2,3,4,5,7,8,9,10,11]\n",
    "\n",
    "# sampling frequency of system (fs=1000 for wandmini)\n",
    "fs = 1000\n",
    "\n",
    "# plot eeg data that is read in\n",
    "plot_raw_data_enable = False\n",
    "\n",
    "# call read in ear eeg\n",
    "all_raw_data, filenames, data_lengths, file_users, refs = read_in_ear_eeg.read_in_clean_data(details_spreadsheet, data_filepath, input_users, data_chs, fs, plot_raw_data_enable)\n",
    "#all_raw_data = np.array(all_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63f0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_data[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510b7c84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# READ-IN LABELS\n",
    "#################\n",
    "\n",
    "# Note: label read in will match Ear EEG read in\n",
    "# (same trials will be read in, and the experiment lengths will be the same)\n",
    "\n",
    "# file path to labels(must be formated r'filepath\\\\')\n",
    "#label_filepath = r'C:\\Users\\Carolyn\\OneDrive\\Documents\\school\\berkeley\\research\\ear_eeg_classification_framework\\experimental_recordings\\drowsiness_studies\\labels\\\\'\n",
    "label_filepath = ear_eeg_base_path + 'eeg_classification_data/ear_eeg_data/labels//'\n",
    "\n",
    "# plot the labels that are read in\n",
    "plot_labels_enable = False\n",
    "\n",
    "# call read in labels\n",
    "all_labels = read_in_labels.read_in_labels(filenames, data_lengths, label_filepath, plot_labels_enable)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "003fc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d665100",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = eeg_filter.filter_studies(all_raw_data)\n",
    "\n",
    "\n",
    "print(len(all_raw_data))\n",
    "print(all_raw_data[0].shape)\n",
    "\n",
    "print(len(filtered_data))\n",
    "print(filtered_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8646cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer have a need for the original raw data so we delete\n",
    "del all_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1185545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data constants\n",
    "carolyn_indices = [0,1,2,3,4]\n",
    "ryan_indices = [5,6,7,8,9]\n",
    "justin_indices = [10,11,12,13,14]\n",
    "conor_indices = [15,16,17,18,19]\n",
    "avi_indices = [20,21]\n",
    "train_perc, val_perc, test_perc = 0.55, 0.30, .15\n",
    "train_ind = [2,3,4,8,9,12,13,14,15,17,18,19,21]\n",
    "val_ind = [1,6,11,16,20,7]\n",
    "test_ind = [0,5,10]\n",
    "\n",
    "# Model Constants\n",
    "window_size = 10 # Seconds\n",
    "sample_rate = 1000 # Hertz\n",
    "\n",
    "# Model Params\n",
    "seq_size = window_size * sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c58005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up into train, val, and test datasets\n",
    "train_data, val_data, test_data = [],[],[]\n",
    "train_labels, val_labels, test_labels = [],[],[]\n",
    "\n",
    "\n",
    "for i in range(len(filtered_data)):\n",
    "    if i in train_ind:\n",
    "        train_data.append(filtered_data[i])\n",
    "        train_labels.append(all_labels[i])\n",
    "    elif i in val_ind:\n",
    "        val_data.append(filtered_data[i])\n",
    "        val_labels.append(all_labels[i])\n",
    "    else:\n",
    "        test_data.append(filtered_data[i])\n",
    "        test_labels.append(all_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62056e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0].T[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa81cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02718772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatt_data(data_set, seq_len):\n",
    "    index_sample_count_map = OrderedDict()\n",
    "    # Data needs to be input as (samples, channels), for ex: (2,400,000, 10)\n",
    "    formatted_datasets = []\n",
    "    for i in range(len(data_set)):\n",
    "        data = data_set[i]\n",
    "        data_length = data.shape[0]\n",
    "        num_seqs = int(data_length/seq_len)\n",
    "        \n",
    "        index_sample_count_map[i] = num_seqs\n",
    "        \n",
    "        formatted_data = np.array(np.split(data, num_seqs))\n",
    "        formatted_datasets.append(formatted_data[:,:128,:]) # 256 is for reducing len TEMPORARILY\n",
    "    return formatted_datasets, index_sample_count_map\n",
    "\n",
    "def one_hot_encode(input):\n",
    "    one_hot_formatted_seq_labels = []\n",
    "    b = np.zeros((int(input.size), int(input.max() + 1)))\n",
    "    b[np.arange(input.size), input] = 1\n",
    "    one_hot_labels = np.array(b)\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "def formatt_labels(labels_set, seq_len):\n",
    "    formatted_labels = []\n",
    "    for i in range(len(labels_set)):\n",
    "        labels = labels_set[i]\n",
    "        old_one_hot_labels = one_hot_encode(labels)\n",
    "        labels_length = old_one_hot_labels.shape[0]\n",
    "        num_seqs = int(labels_length/seq_len)\n",
    "        \n",
    "\n",
    "        new_labels = np.array(np.split(old_one_hot_labels[:num_seqs*seq_len], num_seqs))\n",
    "        formatted_labels.append(new_labels[:,:128,:])\n",
    "    return formatted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea84fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(train_data).shape)\n",
    "print(np.array(train_data).T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bfcb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data\n",
    "proc_train_X, train_seq_count_map = formatt_data(train_data, seq_size)\n",
    "#proc_val_X, val_seq_count_map = formatt_data(val_data, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd84720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the labels\n",
    "proc_train_y = formatt_labels(train_labels, seq_size)\n",
    "\n",
    "#proc_val_y = formatt_labels(val_labels, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bac743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proc_train_X[0].shape)\n",
    "proc_train_y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2379a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as tensors (THIS DATA IS FORMATTED REALLY POORLY, fix after the model stops getting overloaded)\n",
    "batch_ids = []\n",
    "preproc_path = os.path.join(ear_eeg_base_path, \"saved_tensors\")\n",
    "os.makedirs(preproc_path, exist_ok=True)\n",
    "for index in range(len(proc_train_X)):\n",
    "    batch_id = preproc_path + '/recording_' + str(index) + '.pt'\n",
    "    batch_ids.append(index)\n",
    "\n",
    "    torch.save(proc_train_X[index], batch_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85eae030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ME INCORPORATE THE ADDITION OF THE MAP's\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_path, index_sample_count_map, labels):\n",
    "        'Initialization'\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.list_IDs = index_sample_count_map.keys()\n",
    "        self.index_sample_count_map = index_sample_count_map\n",
    "        \n",
    "        recording_global_indices = []\n",
    "        total_count = 0\n",
    "        for recording_len in index_sample_count_map.values():\n",
    "            total_count += recording_len\n",
    "            recording_global_indices.append(total_count)\n",
    "        \n",
    "        self.recording_global_indices = recording_global_indices\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        \n",
    "        return sum(self.recording_global_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        recording_lens = self.index_sample_count_map.values()\n",
    "        \n",
    "        recording_index = 0\n",
    "        lower_bound = 0\n",
    "        upper_bound = 0\n",
    "        \n",
    "        for i in range(len(self.recording_global_indices)):\n",
    "            upper_bound = self.recording_global_indices[i]\n",
    "            if index >= lower_bound and index < upper_bound:\n",
    "                recording_index = i\n",
    "                break\n",
    "            lower_bound = upper_bound\n",
    "            \n",
    "        full_recording_X = torch.load(self.data_path + '/recording_' + str(recording_index) + '.pt')\n",
    "        full_recording_y = self.labels[recording_index]\n",
    "        \n",
    "        inside_recording_index = index - sum(self.recording_global_indices[:recording_index])\n",
    "\n",
    "        X = full_recording_X[inside_recording_index]\n",
    "        y = full_recording_y[inside_recording_index]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dafe01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "BATCH_SIZE = 256\n",
    "SHUFFLE = False\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "# Datasets\n",
    "partition = train_seq_count_map# IDs\n",
    "labels = proc_train_y# Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(preproc_path, partition, labels)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE, \n",
    "                                                 shuffle=SHUFFLE, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "validation_set = Dataset(partition['validation'], labels)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "085ff5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, batch_firsty):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(1000, hid_dim)  # position embedding\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(hid_dim, n_heads, pf_dim, dropout, batch_first=batch_firsty)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [src_len, batch_size]\n",
    "        \n",
    "        # create position tensor\n",
    "        #pos = torch.arange(0, src.shape[0]).unsqueeze(1).repeat(1, src.shape[1]).to(src.device)\n",
    "        \n",
    "        # embed tokens and positions\n",
    "        #tok_embedded = self.dropout(self.tok_embedding(src))  # [src_len, batch_size, hid_dim]\n",
    "        #pos_embedded = self.dropout(self.pos_embedding(pos))  # [src_len, batch_size, hid_dim]\n",
    "        #embedded = tok_embedded + pos_embedded\n",
    "        \n",
    "        # encode sequence\n",
    "        \n",
    "        print(type(src))\n",
    "        \n",
    "        encoded = self.encoder(src.float())  # [src_len, batch_size, hid_dim]\n",
    "        \n",
    "        # get final output and apply linear layer\n",
    "        final_output = encoded.mean(dim=0)  # [batch_size, hid_dim]\n",
    "        logits = self.fc(final_output)  # [batch_size, output_dim]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7783aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "INPUT_DIM = 10 #Not needed we're not embedding\n",
    "OUTPUT_DIM = 2\n",
    "HID_DIM = INPUT_DIM\n",
    "N_LAYERS = 4\n",
    "N_HEADS = 2\n",
    "PF_DIM = 256\n",
    "DROPOUT = 0.1\n",
    "BATCH_FIRST = True # True: (batch, seq, feature). False: (seq, batch, feature)\n",
    "\n",
    "# create model instance\n",
    "model = TransformerClassifier(INPUT_DIM, OUTPUT_DIM, HID_DIM, N_LAYERS, N_HEADS, PF_DIM, DROPOUT, BATCH_FIRST)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# define training and evaluation functions\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for src, trg in iterator:        \n",
    "        print(\"src shape: \", src.shape)\n",
    "        print(\"src shape: \", trg.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "        print(\"output:\", output.shape)\n",
    "        print(\"target:\", trg.shape)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src, trg = batch\n",
    "            output = model(src)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a03d9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training loop\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "    train_loss = train(model, training_generator, optimizer, criterion)\n",
    "#     valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "#     if valid_loss < best_valid_loss:\n",
    "#         best_valid_loss = valid_loss\n",
    "#         torch.save(model.state_dict(), 'model.pt')\n",
    "#     print(f'Epoch {epoch+1}: train loss={train_loss:.3f}, valid loss={valid_loss:.3f}')\n",
    "\n",
    "# load best model and evaluate on test set\n",
    "\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'Test loss={test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8007c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
