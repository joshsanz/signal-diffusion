# Example configuration for DiT training on the Oxford Flowers dataset.

[settings]
config = "~/git/signal-diffusion/config/default.toml"
trainer = "diffusion"

[dataset]
name = "/data/data/signal-diffusion/flowers-102-categories"
train_split = "train"
val_split = ""
batch_size = 8
num_workers = 4
resolution = 64
center_crop = true
random_flip = true
image_column = "image"
caption_column = ""
num_classes = 0

[model]
name = "dit"
sample_size = 64
conditioning = "none" # none, caption, classes

[model.extras]
num_attention_heads = 16
attention_head_dim = 64
in_channels = 3
out_channels = 3
num_layers = 12
patch_size = 2
latent_space = false
num_classes = 0
cfg_dropout = 0.1

[model.lora]
enabled = false

[objective]
prediction_type = "vector_field"
num_timesteps = 1000

[optimizer]
name = "adamw"
learning_rate = 1e-3
weight_decay = 2e-4
betas = [0.9, 0.99]

[scheduler]
name = "constant_with_warmup"
warmup_steps = 100

[training]
seed = 42
epochs = 50
gradient_accumulation_steps = 2
log_every_steps = 10
checkpoint_interval = 1000
checkpoint_total_limit = 3
mixed_precision = "bf16"
gradient_clip_norm = 1.0
snr_gamma = 5.0
eval_strategy = "epoch" # or "epoch"
eval_num_steps = 500
eval_num_examples = 8
eval_gen_seed = 1234
eval_batch_size = 256
eval_mmd_samples = 1000
eval_mmd_fallback_ntrain = 4096

[logging]
tensorboard = true
log_dir = "runs/diffusion/tensorboard"
run_name = "flowers_testing"
# wandb_project = "your-project-name"

[inference]
denoising_steps = 20
cfg_scale = 3.5
