# Example configuration for training the Hourglass transformer denoiser on Oxford Flowers.

[settings]
config = "~/git/signal-diffusion/config/default.toml"
trainer = "diffusion"

[dataset]
name = "nkirschi/oxford-flowers"
train_split = "train"
val_split = "test"
batch_size = 32
num_workers = 4
resolution = 128
center_crop = true
random_flip = true
image_column = "image"
class_column = "label"
num_classes = 102

[model]
name = "hourglass"
sample_size = 128
conditioning = "classes"

[model.extras]
mapping_width = 256
mapping_depth = 2
mapping_d_ff = 768
mapping_cond_dim = 0
mapping_dropout_rate = 0.0
depths = [4, 4, 4]
widths = [128, 384, 768]
d_ffs = [768, 768, 768]
self_attns = [
    { type = "neighborhood", d_head = 64, kernel_size = 7 },
    { type = "neighborhood", d_head = 64, kernel_size = 7 },
    { type = "global", d_head = 64 }
]
dropout_rate = [0.1, 0.1, 0.1]
augment_wrapper = false
augment_prob = 0.0
patch_size = [2, 2]
in_channels = 3
out_channels = 3
cfg_dropout = 0.0

[model.lora]
enabled = false

[objective]
prediction_type = "vector_field"

[optimizer]
name = "adamw"
learning_rate = 1e-3
weight_decay = 1e-4
betas = [0.9, 0.99]

[scheduler]
name = "constant_with_warmup"
warmup_steps = 100

[training]
seed = 42
epochs = 200
gradient_accumulation_steps = 2
log_every_steps = 10
checkpoint_interval = 500
checkpoint_total_limit = 3
mixed_precision = "bf16"
gradient_clip_norm = 1.0
eval_strategy = "steps"
eval_num_steps = 1000
eval_num_examples = 8
eval_gen_seed = 1234
eval_batch_size = 256
eval_mmd_samples = 1000
eval_mmd_fallback_ntrain = 5000

[logging]
tensorboard = true
log_dir = "runs/diffusion/tensorboard"
run_name = "flowers_hourglass"

[inference]
denoising_steps = 30
cfg_scale = 1.0
