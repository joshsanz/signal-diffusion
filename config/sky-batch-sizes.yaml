resources:
  # Optional; if left out, automatically pick the cheapest cloud.
  infra: lambda/us-west-2
  # 1x NVIDIA A100 GPU
  accelerators: A100:1
  autostop:
    idle_minutes: 5
    down: true

file_mounts:
  /data:
    source: s3://signal-diffusion/
    mode: MOUNT
  ~/.netrc: ~/.netrc
  ~/.aws: ~/.aws
  ~/.cache/huggingface/token: ~/.cache/huggingface/token

# Working directory (optional) containing the project codebase.
# Its contents are synced to ~/sky_workdir/ on the cluster.
workdir: .

# Typical use: pip install -r requirements.txt
# Invoked under the workdir (i.e., can use its files).
setup: |
  echo "Running setup."
  curl -LsSf https://astral.sh/uv/install.sh | sh
  cp pyproject.toml.sky pyproject.toml
  uv venv --python=3.12
  uv pip install --upgrade pip setuptools
  uv sync
  # We have pre-built wheels stored in /data/wheels for faster installation of gpu-specific packages without wheels distributed for our target environment.
  uv pip install /data/wheels/$(uname -m)/*.whl
  mkdir -p /data/logs

# Typical use: make use of resources, such as running training.
# Invoked under the workdir (i.e., can use its files).
run: |
  echo "Hello, SkyPilot!"
  export RUN_ID=$(date +%Y%m%d_%H%M%S)
  source .venv/bin/activate
  ./scripts/prep-configs-for-sky.sh
  python ./scripts/edit_config.py -c config/diffusion/*.toml -s training.initial_eval=false
  # Enable VAE tiling otherwise batch sizes are too small
  python ./scripts/edit_config.py -c config/diffusion/localmamba-*.toml config/diffusion/sd35-*.toml -s model.vae_tiling=true
  python ./scripts/find_max_batch_size.py --max-batch-size 256 \
    config/diffusion/localmamba-db-iq.toml | tee /data/logs/find_max_batch_size_localmamba.log
  python ./scripts/find_max_batch_size.py --max-batch-size 256 \
    config/diffusion/hourglass-db-iq.toml | tee /data/logs/find_max_batch_size_hourglass.log
  python ./scripts/find_max_batch_size.py --max-batch-size 256 \
    config/diffusion/sd35-db-iq.toml | tee /data/logs/find_max_batch_size_sd35.log
  # python ./scripts/find_max_model_size.py --batch-size 8
  # ./scripts/test_configs.sh 2>&1 | tee /data/logs/test_configs_${RUN_ID}.log
  echo "Run completed."
